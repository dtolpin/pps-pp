{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import numpy.random\n",
    "\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "\n",
    "import pyro\n",
    "from pyro import sample, param\n",
    "from pyro.optim import SGD, Adam\n",
    "from pyro.infer import EmpiricalMarginal, Importance, SVI, TraceGraph_ELBO\n",
    "from pyro.infer.mcmc import MCMC, HMC\n",
    "from pyro.distributions import Exponential, Bernoulli, Gamma, LogNormal, Normal, Poisson\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_PAGE_COUNT = 10\n",
    "PAGES_PER_SESSION_PRIOR = 5\n",
    "NUMBER_OF_SESSIONS = 100\n",
    "\n",
    "DECAY = 2.\n",
    "\n",
    "# Simulate some trend\n",
    "TREND = PAGES_PER_SESSION_PRIOR * numpy.exp(- numpy.arange(NUMBER_OF_SESSIONS) * DECAY \n",
    "                                            / NUMBER_OF_SESSIONS) \n",
    "         \n",
    "\n",
    "# Sample data around the trend\n",
    "DATA = numpy.minimum(TOTAL_PAGE_COUNT, \n",
    "                          numpy.maximum(1, \n",
    "                                        numpy.round(numpy.random.exponential(TREND))))\n",
    "\n",
    "print(\"Trend from {:.2f} to {:.2f}\".format(TREND[0], TREND[-1]))\n",
    "print(\"Data:\", DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the model of clicking through a campaign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSISTENT = False\n",
    "\n",
    "def persistent_update_beliefs(beliefs, i, j, bandwidth):\n",
    "    update = torch.zeros(beliefs.shape)\n",
    "    update[i, j] = 1\n",
    "    beliefs = beliefs + update\n",
    "    evidence = beliefs[i, :].sum()\n",
    "    if evidence > bandwidth:\n",
    "        scale = torch.ones(beliefs.shape)\n",
    "        scale[i, :] = bandwidth / evidence\n",
    "        beliefs = beliefs * scale\n",
    "    return beliefs\n",
    "\n",
    "\n",
    "def fast_update_beliefs(beliefs, i, j, bandwidth):\n",
    "    beliefs[i, j] += 1\n",
    "    evidence = beliefs[i, :].sum()\n",
    "    if evidence > bandwidth:\n",
    "        beliefs[i, :] *= bandwidth / evidence\n",
    "    return beliefs\n",
    "        \n",
    "    \n",
    "def model(total_page_count, data):\n",
    "    update_beliefs = persistent_update_beliefs if PERSISTENT \\\n",
    "        else fast_update_beliefs\n",
    "    bandwidth = pyro.sample(\"bandwidth\", Exponential(torch.tensor(0.05)))\n",
    "    churn_probability = 1 / PAGES_PER_SESSION_PRIOR\n",
    "    churn_beliefs = torch.stack([2 * churn_probability * torch.ones(total_page_count - 1),\n",
    "                                 2 * (1 - churn_probability) * torch.ones(total_page_count - 1)],\n",
    "                                dim=1)\n",
    "\n",
    "    for (id, pps) in enumerate(data):\n",
    "        for ip in range(total_page_count - 1):\n",
    "            a, b = churn_beliefs[ip]\n",
    "            dist = Bernoulli(probs=a/(a + b))\n",
    "            if ip == int(pps) - 1:\n",
    "                # churned out\n",
    "                sample(\"obs_{}_{}\".format(id, ip), dist, obs=torch.tensor(1.))\n",
    "                churn_beliefs = update_beliefs(churn_beliefs, ip, 0, bandwidth)\n",
    "                break\n",
    "            else:\n",
    "                # stayed \n",
    "                sample(\"obs_{}_{}\".format(id, ip), dist, obs=torch.tensor(0.))\n",
    "                churn_beliefs = update_beliefs(churn_beliefs, ip, 1, bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance sampling\n",
    "\n",
    "Slow but should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "def sample_posterior(model, n=N):\n",
    "    posterior = Importance(model, num_samples=n)\n",
    "    marginal = EmpiricalMarginal(posterior.run(TOTAL_PAGE_COUNT, DATA), \n",
    "                                     sites=\"bandwidth\")\n",
    "    bandwidth_samples = [marginal().item() for _ in range(n)]\n",
    "    return bandwidth_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for p in [False, True]:\n",
    "    global PERSISTENT\n",
    "    PERSISTENT = p\n",
    "    label = \"PERSISTENT = {}\".format(PERSISTENT)\n",
    "    print(label)\n",
    "    %time imp_bandwidth_samples = sample_posterior(model)\n",
    "    plt.hist(imp_bandwidth_samples, density=True, label=label, alpha=0.75)\n",
    "    print(\"{:.4f} {:.4f}\"\n",
    "          .format(numpy.mean(imp_bandwidth_samples),\n",
    "                  numpy.std(imp_bandwidth_samples)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUTS (Hybrid Monte Carlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_kernel = HMC(model)\n",
    "mcmc_run = MCMC(hmc_kernel, num_samples=500, warmup_steps=100).run(TOTAL_PAGE_COUNT, DATA)\n",
    "posterior = EmpiricalMarginal(mcmc_run, 'bandwidth')\n",
    "posterior.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "The guide (the posterior distribution of the bandwidth) is a Gamma distribution. We parameterize it by mean and standard deviation. It does not really matter how we parameterize it except for ease of convergence. This works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(total_page_count, data):\n",
    "    mu = pyro.param(\"mu\", torch.tensor(20.0), \n",
    "                    constraint=constraints.positive)\n",
    "    sigma = pyro.param(\"sigma\", torch.tensor(5.0),\n",
    "                       constraint=constraints.positive)\n",
    "    alpha = mu*mu / (sigma * sigma)\n",
    "    beta = alpha / mu\n",
    "    pyro.sample(\"bandwidth\", Gamma(alpha, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_params = {\"lr\": 0.01}\n",
    "optimizer = Adam(optim_params)\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(model, guide=guide, optim=optimizer, loss=TraceGraph_ELBO())\n",
    "\n",
    "# clear the param store\n",
    "pyro.clear_param_store()\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do gradient steps\n",
    "PERSISTENT = True\n",
    "best_mu = None\n",
    "best_sigma = None\n",
    "min_loss = numpy.nan\n",
    "for step in range(100):\n",
    "    loss = svi.step(TOTAL_PAGE_COUNT, DATA)\n",
    "    losses.append(loss)\n",
    "    if not (loss >= min_loss):\n",
    "        best_mu = pyro.param(\"mu\").item()\n",
    "        best_sigma = pyro.param(\"sigma\").item()\n",
    "        print(\"{:4d}: loss={:.1f} => {:.1f} mean={:.4f} std={:.4f}\"\n",
    "              .format(step + 1, min_loss, loss, best_mu, best_sigma))\n",
    "        min_loss = loss\n",
    "    else:\n",
    "        print(\"{:4d}: loss={:.1f} mean={:.4f} std={:.4f}\"\n",
    "              .format(step + 1, min_loss, best_mu, best_sigma), end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the best parameters\n",
    "pyro.clear_param_store()\n",
    "pyro.param(\"mu\", torch.tensor(best_mu))\n",
    "pyro.param(\"sigma\", torch.tensor(best_sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WINDOW = len(losses) // 10\n",
    "plt.plot([numpy.mean(losses[i:i + WINDOW]) for i in range(len(losses) - WINDOW)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi_bandwidth_samples = sample_posterior(guide)\n",
    "plt.hist(imp_bandwidth_samples, density=True, label=\"Importance\", alpha=0.75)\n",
    "plt.hist(svi_bandwidth_samples, density=True, label=\"SVI\", alpha=0.75)\n",
    "plt.legend()\n",
    "print(\"{:.4f} {:.4f}\"\n",
    "      .format(numpy.mean(svi_bandwidth_samples),\n",
    "              numpy.std(svi_bandwidth_samples)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
